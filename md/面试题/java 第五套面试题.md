# java 第五套面试题

## Java类加载机制是什么？



Java虚拟机加载类的全过程包括：加载、验证、准备、解析、初始化。验证、准备、解析叫连接过程。今天我们讲加载。
首先明确“加载”是“类加载”的一个过程，不要混淆。加载过程中，虚拟机需要完成下列三个事情：

通过一个类的全限定名获取定义此类的二进制字节流；
将这个字节流所代表的静态存储结构转化为方法区运行时的数据结构；
在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。
虚拟机的这三点要求不算具体，因此虚拟机实现与具体应用的灵活度都是相当大的。就拿第一条说，他仅仅要求“通过一个类的全限定名获取定义此类的二进制字节流”，并没有指明二进制字节流要从Class文件获取，也没说要怎样获取，从哪里获取。所以说java虚拟机的设计团队在加载阶段搭建了一个很广阔的舞台，许多举足轻重的java技术都建立在这个舞台上，比如：

从ZIP包中获取，这是jar，war格式的基础
从网络中获取，这种场景的典型应用就是Applet
运行时计算生成，这种场景使用的最多的就是动态代理技术，在java.lang.reflect.Proxy中，就是用了ProxyGenerator.generateProxyClass来为特定接口生成形式为“*$Proxy”的代理类的二进制字节流
由其他文件生成，典型的场景是JSP应用，即JSP文件生成对应的Class类
从数据库中读取，这种场景相对少见
相对于类加载过程的其他阶段，一个非数组类的加载阶段（准确的说，是加载阶段中获取二进制字节流的动作，有的加载阶段不获取二进制字节流）是开发人员 可控性最强的，因为加载阶段即可以使用系统提供的引导类加载器来完成，也可以由用户自定义的类加载器去完成，开发人员可以通过定义自己的类加载器去控制字节流的获取方式（即重写一个类的loadClass方法）。

对于数组类而言，情况不同。数组类本身不通过类加载器创建，他是由java虚拟机直接创建。但数组类与类加载器的关系任然很密切，因为数组类的元素类型（指的是数组去掉所有维度的类型）最终要靠类加载器创建，一个数组类（下面简称C）创建过程遵循以下规则：

如果数组的组件类型（指的是数组去掉一个维度的类型）是引用类型，那就递归采用本节中定义的加载过程去加载这个组件类型，数组C将在加载该类型组件的类加载器的类名称空间上被标识（这点很重要，一个类必须与类加载器一起确定唯一性）
如果数组的组件类型不是引用类型（例如int[]数组），Java虚拟机将会把数组C标记为与引导类加载器关联
数组类的可见性与他的组件类型的可见性一致，如果组件类型不是引用类型，那数组类的可见性将默认为public
加载阶段完成之后，虚拟机外部的二进制字节流就按照虚拟机所需的格式存储在方法区之中，方法区中的数据存储格式由虚拟机实现自行定义，虚拟机规范未规定此区域的具体数据结构。然后在内存中实例化一个java.lang.Class类的对象（并没有明确规定是在java堆中，对于HotSpot虚拟机而言，Class对象比较特殊，他虽然是对象，但是放在方法区中），这个对象将作为程序访问方法区中的这些类型数据的外部接口。
加载阶段与连接阶段的部分内容（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能开始了，但是这些夹在加载阶段之中进行的动作，任然属于连接阶段的内容，这两个阶段的开始时间任然保持着固定的先后顺序。

## Java垃圾回收机制是什么？

java的垃圾回收机制是java语言的重要功能之一。当程序创建对象，数组等引用类型的实体时，Java就在堆中为其分配一块内存区，对象就保存在这块区域中，当这块内存不再被任何引用变量引用时，这块内存就变成了垃圾，等待垃圾回收机制进行回收。

垃圾回收机制具有以下等特征：

​	-> 垃圾回收机制只会回收堆内存中的垃圾,不会回收任何的物理资源(例如数据库的连接,资源的配置等)。

​	->程序无法精准的控制垃圾回收的运行,垃圾回收会在合适的时候进行.当对象永久性地失去引用后,系统就会在	合适的时候收回它所占的内存。

​	->垃圾回收机制在回收任何对象之前，总会先调用它的finalize()方法，该方法可能会使该对象重新复活(让一··	个引用变量重新引用该对象)，从而导致垃圾回收机制取消回收。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  



对象在内存中的状态决定了它是否要被垃圾回收机制回收。

当一个对象在堆内存中运行时，根据它被引用变量所引用的状态，可以把它所处的状态分为三种：

可达状态：当一个对象被创建后，若有一个及以上的引用变量引用他，则这个对象在程序中处于可达状态，

程序可通过引用变量调用该对象的实例变量和方法。

可恢复状态：如果程序中的某个对象不再被任何变量引用时，他就进入了可恢复状态。在这种状态下，系统的垃圾回收机制准备回收它所占的内存。在收回该对象前，系统会调用所有可恢复对象的finalize()方法进行资源的清理。如果系统调用finalize()方法的时候重新让一个引用变量指向了该对象，该对象的状态就会变为可达状态，取消对其执行垃圾回收机制。否则对象将会进入不可达状态。

不可达状态：

当对象与所有引用变量之间的关系都被切断，且系统已经调用过finalize()方法，对象依然没有转换为可达状态

，这时对象将永久的失去引用，然后被垃圾回收机制回收。



![1578489265950](img\1578489265950.png)



## LinkList和ArrayList的区别是什么？

考察点：ArrayList
参考回答：
----ArrayList 和 LinkedList 都实现了 List 接口，他们有以下的不同点：
---- ArrayList 是基于索引的数据接口，它的底层是数组。它可以以 O(1)时间复杂度对元素进行随机 访问。
---- LinkedList 是以元素列表的形式存储它的数据，每一个元素都和它的前一个 和后一个元素链接在一起，在这种情况下，查找某个元素的时间复杂度是 O(n)。
---- 相对于 ArrayList，LinkedList 的插入，添加，删除操作速度更快，因为当元素被添加到集合任 意位置的时候，不需要像数组那样重新计算大小或者是更新索引。
----LinkedList 比 ArrayList 更占内存，因为 LinkedList 为每一个节点存储了两个引用，一个指向 前一个元素，一个指向下一个元素。

## HashMap的实现原理是什么？

①HashMap的工作原理

HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。

当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的链表中。键对象的equals()方法用来找到键值对。

因为HashMap的好处非常多，我曾经在电子商务的应用中使用HashMap作为缓存。因为金融领域非常多的运用Java，也出于性能的考虑，我们会经常用到HashMap和ConcurrentHashMap。

### ②HashMap和Hashtable的区别

HashMap和Hashtable都实现了Map接口，但决定用哪一个之前先要弄清楚它们之间的分别。主要的区别有：线程安全性，同步(synchronization)，以及速度。

1. HashMap几乎可以等价于Hashtable，除了HashMap是非synchronized的，并可以接受null(HashMap可以接受为null的键值(key)和值(value)，而Hashtable则不行)。
2. HashMap是非synchronized，而Hashtable是synchronized，这意味着Hashtable是线程安全的，多个线程可以共享一个Hashtable；而如果没有正确的同步的话，多个线程是不能共享HashMap的。Java 5提供了ConcurrentHashMap，它是HashTable的替代，比HashTable的扩展性更好。
3. 另一个区别是HashMap的迭代器(Iterator)是fail-fast迭代器，而Hashtable的enumerator迭代器不是fail-fast的。所以当有其它线程改变了HashMap的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的remove()方法移除元素则不会抛出ConcurrentModificationException异常。但这并不是一个一定发生的行为，要看JVM。这条同样也是Enumeration和Iterator的区别。
4. 由于Hashtable是线程安全的也是synchronized，所以在单线程环境下它比HashMap要慢。如果你不需要同步，只需要单一线程，那么使用HashMap性能要好过Hashtable。
5. HashMap不能保证随着时间的推移Map中的元素次序是不变的。

### 要注意的一些重要术语：

1) sychronized意味着在一次仅有一个线程能够更改Hashtable。就是说任何线程要更新Hashtable时要首先获得同步锁，其它线程要等到同步锁被释放之后才能再次获得同步锁更新Hashtable。

2) Fail-safe和iterator迭代器相关。如果某个集合对象创建了Iterator或者ListIterator，然后其它的线程试图“结构上”更改集合对象，将会抛出ConcurrentModificationException异常。但其它线程可以通过set()方法更改集合对象是允许的，因为这并没有从“结构上”更改集合。但是假如已经从结构上进行了更改，再调用set()方法，将会抛出IllegalArgumentException异常。

3) 结构上的更改指的是删除或者插入一个元素，这样会影响到map的结构。

### 我们能否让HashMap同步？

HashMap可以通过下面的语句进行同步：
Map m = Collections.synchronizeMap(hashMap);

### 结论

Hashtable和HashMap有几个主要的不同：线程安全以及速度。仅在你需要完全的线程安全的时候使用Hashtable，而如果你使用Java 5或以上的话，请使用ConcurrentHashMap吧。

ashMap和HashSet的区别是Java面试中最常被问到的问题。如果没有涉及到Collection框架以及多线程的面试，可以说是不完整。而Collection框架的问题不涉及到HashSet和HashMap，也可以说是不完整。HashMap和HashSet都是collection框架的一部分，它们让我们能够使用对象的集合。collection框架有自己的接口和实现，主要分为Set接口，List接口和Queue接口。它们有各自的特点，Set的集合里不允许对象有重复的值，List允许有重复，它对集合中的对象进行索引，Queue的工作原理是FCFS算法(First Come, First Serve)。

首先让我们来看看什么是HashMap和HashSet，然后再来比较它们之间的分别。

### ③HashMap和HashSet的区别

HashMap和HashSet的区别是Java面试中最常被问到的问题。如果没有涉及到Collection框架以及多线程的面试，可以说是不完整。而Collection框架的问题不涉及到HashSet和HashMap，也可以说是不完整。HashMap和HashSet都是collection框架的一部分，它们让我们能够使用对象的集合。collection框架有自己的接口和实现，主要分为Set接口，List接口和Queue接口。它们有各自的特点，Set的集合里不允许对象有重复的值，List允许有重复，它对集合中的对象进行索引，Queue的工作原理是FCFS算法(First Come, First Serve)。

首先让我们来看看什么是HashMap和HashSet，然后再来比较它们之间的分别。

### 什么是HashSet

HashSet实现了Set接口，它不允许集合中有重复的值，当我们提到HashSet时，第一件事情就是在将对象存储在HashSet之前，要先确保对象重写equals()和hashCode()方法，这样才能比较对象的值是否相等，以确保set中没有储存相等的对象。如果我们没有重写这两个方法，将会使用这个方法的默认实现。

public boolean add(Object o)方法用来在Set中添加元素，当元素值重复时则会立即返回false，如果成功添加的话会返回true。

### 什么是HashMap

HashMap实现了Map接口，Map接口对键值对进行映射。Map中不允许重复的键。Map接口有两个基本的实现，HashMap和TreeMap。TreeMap保存了对象的排列次序，而HashMap则不能。HashMap允许键和值为null。HashMap是非synchronized的，但collection框架提供方法能保证HashMap synchronized，这样多个线程同时访问HashMap时，能保证只有一个线程更改Map。

public Object put(Object Key,Object value)方法用来将元素添加到map中。

### HashSet和HashMap的区别

| *HashMap*                                   | *HashSet*                                                    |
| ------------------------------------------- | ------------------------------------------------------------ |
| HashMap实现了Map接口                        | HashSet实现了Set接口                                         |
| HashMap储存键值对                           | HashSet仅仅存储对象                                          |
| 使用put()方法将元素放入map中                | 使用add()方法将元素放入set中                                 |
| HashMap中使用键对象来计算hashcode值         | HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性，如果两个对象不同的话，那么返回false |
| HashMap比较快，因为是使用唯一的键来获取对象 | HashSet较HashMap来说比较慢                                   |

 

 

### ④面试题

HashMap的工作原理是近年来常见的Java面试题。几乎每个Java程序员都知道HashMap，都知道哪里要用HashMap，知道Hashtable和HashMap之间的区别，那么为何这道面试题如此特殊呢？是因为这道题考察的深度很深。这题经常出现在高级或中高级面试中。投资银行更喜欢问这个问题，甚至会要求你实现HashMap来考察你的编程能力。ConcurrentHashMap和其它同步集合的引入让这道题变得更加复杂。让我们开始探索的旅程吧！

“你用过HashMap吗？” “什么是HashMap？你为什么用到它？”

几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而Hashtable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题：

“你知道HashMap的工作原理吗？” “你知道HashMap的get()方法的工作原理吗？”

你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。”

但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法：

“当两个对象的hashcode相同会发生什么？” 从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用链表存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在链表中。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问：

“如果两个键的hashcode相同，你如何获取值对象？” 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历链表直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者直到HashMap在链表中存储的是键值对，否则他们不可能回答出这一题。

其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到链表中正确的节点，最终找到要找的值对象。完美的答案！

许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。

如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。“如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？”除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。

如果你能够回答这道问题，下面的问题来了：“你了解重新调整HashMap大小存在什么问题吗？”你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition)。

当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢



## 简单讲一下工厂模式的优势？

工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。

**使用目的：**定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。

**优点：**(1)一个调用者想创建一个对象，只要知道其名称就可以了。(2)扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以(3)屏蔽产品的具体实现，调用者只关心产品的接口。

**缺点：**(1)每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。

**使用场景：(1)**日志记录器：记录可能记录到本地硬盘、系统事件、远程服务器等，用户可以选择记录日志到什么地方(2)数据库访问，当用户不知道最后系统采用哪一类数据库，以及数据库可能有变化时(3)设计一个连接服务器的框架，需要三个协议，"POP3"、"IMAP"、"HTTP"，可以把这三个作为产品类，共同实现一个接口。



## Spring的事务管理？

1 初步理解
理解事务之前，先讲一个你日常生活中最常干的事：取钱。
比如你去ATM机取1000块钱，大体有两个步骤：首先输入密码金额，银行卡扣掉1000元钱；然后ATM出1000元钱。这两个步骤必须是要么都执行要么都不执行。如果银行卡扣除了1000块但是ATM出钱失败的话，你将会损失1000元；如果银行卡扣钱失败但是ATM却出了1000块，那么银行将损失1000元。所以，如果一个步骤成功另一个步骤失败对双方都不是好事，如果不管哪一个步骤失败了以后，整个取钱过程都能回滚，也就是完全取消所有操作的话，这对双方都是极好的。
事务就是用来解决类似问题的。事务是一系列的动作，它们综合在一起才是一个完整的工作单元，这些动作必须全部完成，如果有一个失败的话，那么事务就会回滚到最开始的状态，仿佛什么都没发生过一样。
在企业级应用程序开发中，事务管理必不可少的技术，用来确保数据的完整性和一致性。

2、事务有四个特性：ACID
原子性（Atomicity）：事务是一个原子操作，由一系列动作组成。事务的原子性确保动作要么全部完成，要么完全不起作用。
一致性（Consistency）：一旦事务完成（不管成功还是失败），系统必须确保它所建模的业务处于一致的状态，而不会是部分完成部分失败。在现实中的数据不应该被破坏。
隔离性（Isolation）：可能有许多事务会同时处理相同的数据，因此每个事务都应该与其他事务隔离开来，防止数据损坏。
持久性（Durability）：一旦事务完成，无论发生什么系统错误，它的结果都不应该受到影响，这样就能从任何系统崩溃中恢复过来。通常情况下，事务的结果被写到持久化存储器中。

3、事务的传播行为
PROPAGATION_REQUIRED 表示当前方法必须运行在事务中。如果当前事务存在，方法将会在该事务中运行。否则，会启动一个新的事务
PROPAGATION_SUPPORTS 表示当前方法不需要事务上下文，但是如果存在当前事务的话，那么该方法会在这个事务中运行
PROPAGATION_MANDATORY 表示该方法必须在事务中运行，如果当前事务不存在，则会抛出一个异常
PROPAGATION_REQUIRED_NEW 表示当前方法必须运行在它自己的事务中。一个新的事务将被启动。如果存在当前事务，在该方法执行期间，当前事务会被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager
PROPAGATION_NOT_SUPPORTED 表示该方法不应该运行在事务中。如果存在当前事务，在该方法运行期间，当前事务将被挂起。如果使用JTATransactionManager的话，则需要访问TransactionManager
PROPAGATION_NEVER 表示当前方法不应该运行在事务上下文中。如果当前正有一个事务在运行，则会抛出异常
PROPAGATION_NESTED 表示如果当前已经存在一个事务，那么该方法将会在嵌套事务中运行。嵌套的事务可以独立于当前事务进行单独地提交或回滚。如果当前事务不存在，那么其行为与PROPAGATION_REQUIRED一样。注意各厂商对这种传播行为的支持是有所差异的。可以参考资源管理器的文档来确认它们是否支持嵌套事务

4、事务的隔离级别
ISOLATION_DEFAULT 使用后端数据库默认的隔离级别
ISOLATION_READ_UNCOMMITTED（读未提交） 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读
ISOLATION_READ_COMMITTED（读已提交） 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生
ISOLATION_REPEATABLE_READ（可重复读） 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生
ISOLATION_SERIALIZABLE（串行化） 最高的隔离级别，完全服从ACID的隔离级别，确保阻止脏读、不可重复读以及幻读，也是最慢的事务隔离级别，因为它通常是通过完全锁定事务相关的数据库表来实现的




## 线程安全集合用过哪些？

1.Vector,Vector是ArrayList的线程安全版本，性能比ArrayList要低，现在已很少使用。Hashtable是线程安全的，现在也很少使用。在jdk1.8之后使用concurrentHashMap这个线程安全的集合比较多。

2.Collections集合工具类的同步集合方法.

3.CopyOnWriteArrayList 采用读写分离的思想,当进行写入删除操作时,底层会把原数组复制一份进行修改,修改操作完成时,再把变量引用指向新的数组,使用ReentrantLock可重入锁对修改操作进行锁定,在一定程度上提高了效率,

但内存消耗为2n,加剧了内存的损耗.

## hash冲突如何解决？

通过构造性能良好的哈希函数，可以减少冲突，但一般不可能完全避免冲突，因此解决冲突是哈希法的另一个关键问题。

　　创建哈希表和查找哈希表都会遇到冲突，两种情况下解决冲突的方法应该一致。

　　下面以创建哈希表为例，说明解决冲突的方法。常用的解决冲突方法有以下四种：

　　**开放定址法**

　　这种方法也称再散列法，其基本思想是：当关键字key的哈希地址**p=H（key）**出现冲突时，以p为基础，产生另一个哈希地址p1，如果p1仍然冲突，再以p为基础，产生另一个哈希地址p2，…，直到找出一个不冲突的哈希地址pi ，将相应元素存入其中。

![img](http://spider.ws.126.net/b616b76584480f902564cfca340da0e2.jpeg)

　这种方法有一个通用的再散列函数形式：

　　

> Hi=（H（key）+di）% m i=1，2，…，n



　　其中H（key）为哈希函数，m 为表长，di称为增量序列。增量序列的取值方式不同，相应的再散列方式也不同。主要有以下三种：

　　线性探测再散列

　　

> dii=1，2，3，…，m-1



　　这种方法的特点是：冲突发生时，顺序查看表中下一单元，直到找出一个空单元或查遍全表。

　　二次探测再散列

　　

> di=12，-12，22，-22，…，k2，-k2 ( k<=m/2 )



　　这种方法的特点是：冲突发生时，在表的左右进行跳跃式探测，比较灵活。

　　伪随机探测再散列，di=伪随机数序列。

　　具体实现时，应建立一个伪随机数发生器，（如**i=(i+p) % m**），并给定一个随机数做起点。

　　例如，已知哈希表长度m=11，哈希函数为：**H（key）= key % 11**，则**H（47）=3，H（26）=4，H（60）=5**，假设下一个关键字为69，则H（69）=3，与47冲突。

　　如果用线性探测再散列处理冲突，下一个哈希地址为**H1=（3 + 1）% 11 = 4**，仍然冲突，再找下一个哈希地址为**H2=（3 + 2）% 11 = 5**，还是冲突，继续找下一个哈希地址为**H3=（3 + 3）% 11 = 6**，此时不再冲突，将69填入5号单元。

　　如果用二次探测再散列处理冲突，下一个哈希地址为**H1=（3 + 12）% 11 = 4**，仍然冲突，再找下一个哈希地址为**H2=（3 - 12）% 11 = 2**，此时不再冲突，将69填入2号单元。

　　如果用伪随机探测再散列处理冲突，且伪随机数序列为：**2，5，9，……..**，则下一个哈希地址为**H1=（3 + 2）% 11 = 5**，仍然冲突，再找下一个哈希地址为**H2=（3 + 5）% 11 = 8**，此时不再冲突，将69填入8号单元。

　　**再哈希法**

　　这种方法是同时构造多个不同的哈希函数：

　　

> Hi=RH1（key） i=1，2，…，k



　　当哈希地址**Hi=RH1（key）**发生冲突时，再计算**Hi=RH2（key）**……，直到冲突不再产生。这种方法不易产生聚集，但增加了计算时间。

　　**链地址法**

　　这种方法的基本思想是将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。

![img](http://spider.ws.126.net/111f15049d9042ebe9c857ad45ed6575.jpeg)

建立公共溢出区

　　这种方法的基本思想是：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。

　　**优缺点**

　　开放散列（**open hashing**）/ 拉链法（针对桶链结构）

　　1）优点：

　　①对于记录总数频繁可变的情况，处理的比较好（也就是避免了动态调整的开销）

　　②由于记录存储在结点中，而结点是动态分配，不会造成内存的浪费，所以尤其适合那种记录本身尺寸（size）很大的情况，因为此时指针的开销可以忽略不计了

　　③删除记录时，比较方便，直接通过指针操作即可

　　2）缺点：

　　①存储的记录是随机分布在内存中的，这样在查询记录时，相比结构紧凑的数据类型（比如数组），哈希表的跳转访问会带来额外的时间开销

　　②如果所有的 key-value 对是可以提前预知，并之后不会发生变化时（即不允许插入和删除），可以人为创建一个不会产生冲突的完美哈希函数（perfect hash function），此时封闭散列的性能将远高于开放散列

　　③由于使用指针，记录不容易进行序列化（serialize）操作

　　**封闭散列（closed hashing）/ 开放定址法**

　　1）优点：

　　①记录更容易进行序列化（serialize）操作

　　②如果记录总数可以预知，可以创建完美哈希函数，此时处理数据的效率是非常高的

　　2）缺点：

　　①存储记录的数目不能超过桶数组的长度，如果超过就需要扩容，而扩容会导致某次操作的时间成本飙升，这在实时或者交互式应用中可能会是一个严重的缺陷

　　②使用探测序列，有可能其计算的时间成本过高，导致哈希表的处理性能降低

　　③由于记录是存放在桶数组中的，而桶数组必然存在空槽，所以当记录本身尺寸（size）很大并且记录总数规模很大时，空槽占用的空间会导致明显的内存浪费

　　④删除记录时，比较麻烦。比如需要删除记录a，记录b是在a之后插入桶数组的，但是和记录a有冲突，是通过探测序列再次跳转找到的地址，所以如果直接删除a，a的位置变为空槽，而空槽是查询记录失败的终止条件，这样会导致记录b在a的位置重新插入数据前不可见，所以不能直接删除a，而是设置删除标记。这就需要额外的空间和操作。



## cas是什么？

CAS（Compare and Swap），即比较并替换，实现并发算法时常用到的一种技术，Doug lea大神在java同步器中大量使用了CAS技术，鬼斧神工的实现了多线程执行的安全性。

CAS的思想很简单：三个参数，一个当前内存值V、旧的预期值A、即将更新的值B，当且仅当预期值A和内存值V相同时，将内存值修改为B并返回true，否则什么都不做，并返回false。



## aqs是什么？

AQS，即AbstractQueuedSynchronizer, 队列同步器，它是Java并发用来构建锁和其他同步组件的基础框架。来看下同步组件对AQS的使用：

AQS是一个抽象类，主是是以继承的方式使用。AQS本身是没有实现任何同步接口的，它仅仅只是定义了同步状态的获取和释放的方法来供自定义的同步组件的使用。从图中可以看出，在java的同步组件中，AQS的子类（Sync等）一般是同步组件的静态内部类，即通过组合的方式使用。

抽象的队列式的同步器，AQS定义了一套多线程访问共享资源的同步器框架，许多同步类实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch

AQS的实现依赖内部的同步队列（FIFO双向队列），如果当前线程获取同步状态失败，AQS会将该线程以及等待状态等信息构造成一个Node，将其加入同步队列的尾部，同时阻塞当前线程，当同步状态释放时，唤醒队列的头节点。

## Java中的软引用、弱引用、虚引用的适用场景以及释放机制是什么？

在Java语言中，除了基本数据类型外，其他的都是指向各类对象的对象引用；Java中根据其生命周期的长短，将引用分为4类。

不同的引用类型，主要体现的是对象不同的可达性状态和对垃圾收集的影响。

1.软引用

软引用通过SoftReference类实现。 软引用的生命周期比强引用短一些。

只有当 JVM 认为内存不足时，才会去试图回收软引用指向的对象，即JVM 会确保在抛出 OutOfMemoryError 之前，清理软引用指向的对象。

软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收器回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。

后续，我们可以调用ReferenceQueue的poll()方法来检查是否有它所关心的对象被回收。如果队列为空，将返回一个null,否则该方法返回队列中前面的一个Reference对象。

应用场景：软引用通常用来实现内存敏感的缓存。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。

2.弱引用

弱引用通过WeakReference类实现。 弱引用的生命周期比软引用短。

不管当前内存空间足够与否，都会回收它的内存。由于垃圾回收器是一个优先级很低的线程，因此不一定会很快回收弱引用的对象。

弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列

应用场景：弱应用同样可用于内存敏感的缓存。

3.虚引用也叫幻象引用，通过PhantomReference类来实现。

如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。

无法通过虚引用访问对象的任何属性或函数。仅仅是提供了一种确保对象被 finalize 以后，做某些事情的机制。

虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。

ReferenceQueue queue = new ReferenceQueue ();

PhantomReference pr = new PhantomReference (object, queue);

程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。

如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取一些程序行动。

应用场景：可用来跟踪对象被垃圾回收器回收的活动，当一个虚引用关联的对象被垃圾收集器回收之前会收到一条系统通知。 

## BIO和NIO的区别以及多路复用机制？

BIO

  读取时如果数据还没准备好，则阻塞线程。

> #### 缺点
>
> - 发生上下文切换，一个线程管理一个io

NIO

  不阻塞，读取时如果还没数据准备好，则返回-1。

> #### 缺点
>
> - 如果都多个io，需要一个一个检测，每次检测调用read都会发生上下文切换（read是系统调用，每调用一次就得在用户态和核心态切换一次）
> - 第一次读取不到时，不知道应该等待多久再尝试读取一次

IO多路复用

  IO多路复用（IO Multiplexing) 是这么一种机制：程序注册一组socket文件描述符给操作系统，表示“我要监视这些fd是否有IO事件发生，有了就告诉程序处理”。
   IO多路复用和NIO是要配合一起使用才有实际意义。
   IO多路复用有select、poll、epoll三种方式。



## CMS GC回收分为哪几个阶段？分别做了什么事情？

我们先回顾一下主流Java的垃圾回收器(HotSpot JVM)。

堆被分解为较小的三个部分。具体分为：新生代、老年代、持久代。

![img](https://upload-images.jianshu.io/upload_images/302647-bc51a1dc8f905636..png)

1. 绝大部分新生成的对象都放在Eden区，当Eden区将满，JVM会因申请不到内存，而触发Young GC ,进行Eden区+有对象的Survivor区(设为S0区)垃圾回收，把存活的对象用复制算法拷贝到一个空的Survivor(S1)中，此时Eden区被清空，另外一个Survivor S0也为空。下次触发Young GC回收Eden+S0，将存活对象拷贝到S1中。新生代垃圾回收简单、粗暴、高效。
2. 若发现Survivor区满了，则将这些对象拷贝到old区或者Survivor没满但某些对象足够Old,也拷贝到Old区(每次Young GC都会使Survivor区存活对象值+1，直到阈值)。 3.Old区也会进行垃圾收集(Young GC),发生一次 Major GC 至少伴随一次Young GC，一般比Young GC慢十倍以上。
3. JVM在Old区申请不到内存，会进行Full GC。Old区使用一般采用Concurrent-Mark–Sweep策略回收内存。

总结：Java垃圾回收器是一种“自适应的、分代的、停止—复制、标记-清扫”式的垃圾回收器。

缺点：

1. GC过程中会出现STW(Stop-The-World)，若Old区对象太多，STW耗费大量时间。
2. CMS收集器对CPU资源很敏感。
3. CMS收集器无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。
4. CMS导致内存碎片问题。

## CMS有哪些重要参数？

**1. UseCMSCompactAtFullCollection 与 CMSFullGCsBeforeCompaction**

有一点需要注意的是：CMS并发GC不是“full GC”。HotSpot VM里对concurrent collection和full collection有明确的区分。所有带有“FullCollection”字样的VM参数都是跟真正的full GC相关，而跟CMS并发GC无关的。

**2. -XX:CMSInitiatingOccupancyFraction=70 和-XX:+UseCMSInitiatingOccupancyOnly**

这两个设置一般配合使用,一般用于『降低CMS GC频率或者增加频率、减少GC时长』的需求

-XX:CMSInitiatingOccupancyFraction=70 是指设定CMS在对内存占用率达到70%的时候开始GC(因为CMS会有浮动垃圾,所以一般都较早启动GC);

-XX:+UseCMSInitiatingOccupancyOnly 只是用设定的回收阈值(上面指定的70%),如果不指定,JVM仅在第一次使用设定值,后续则自动调整.

**3. -XX:+CMSScavengeBeforeRemark**

在CMS GC前启动一次ygc，目的在于减少old gen对ygc gen的引用，降低remark时的开销-----一般CMS的GC耗时 80%都在remark阶段



## 有做过哪些GC调优？

在调优之前，我们需要记住下面的原则：

````txt
多数的 Java 应用不需要在服务器上进行 GC 优化；

多数导致 GC 问题的 Java 应用，都不是因为我们参数设置错误，而是代码问题；

在应用上线之前，先考虑将机器的 JVM 参数设置到最优（最适合）；

减少创建对象的数量；

减少使用全局变量和大对象；

GC 优化是到最后不得已才采用的手段；

在实际使用中，分析 GC 情况优化代码比优化 GC 参数要多得多。
````

GC调优的目的:

````txt
将转移到老年代的对象数量降低到最小；

减少 GC 的执行时间。
````

**策略 1：**将新对象预留在新生代，由于 Full GC 的成本远高于 Minor GC，因此尽可能将对象分配在新生代是明智的做法，实际项目中根据 GC 日志分析新生代空间大小分配是否合理，适当通过“-Xmn”命令调节新生代大小，最大限度降低新对象直接进入老年代的情况。

**策略 2：**大对象进入老年代，虽然大部分情况下，将对象分配在新生代是合理的。但是对于大对象这种做法却值得商榷，大对象如果首次在新生代分配可能会出现空间不足导致很多年龄不够的小对象被分配的老年代，破坏新生代的对象结构，可能会出现频繁的 full gc。因此，对于大对象，可以设置直接进入老年代（当然短命的大对象对于垃圾回收老说简直就是噩梦）。-XX:PretenureSizeThreshold 可以设置直接进入老年代的对象大小。

**策略 3：**合理设置进入老年代对象的年龄，-XX:MaxTenuringThreshold 设置对象进入老年代的年龄大小，减少老年代的内存占用，降低 full gc 发生的频率。

**策略 4：**设置稳定的堆大小，堆大小设置有两个参数：-Xms 初始化堆大小，-Xmx 最大堆大小。

**策略5：**注意： 如果满足下面的指标，**则一般不需要进行 GC 优化：**

````txt
MinorGC 执行时间不到50ms；

Minor GC 执行不频繁，约10秒一次；

Full GC 执行时间不到1s；

Full GC 执行频率不算频繁，不低于10分钟1次。
````



## 年轻代为什么采用的是复制算法？

年轻代对象因为生命周期短，每次有约90%以上对象的占用空间被回收，采用“复制-清除”算法清理，
具体过程：将新生代分为一个Eden空间和两个Survivor空间，默认Eden空间和Survivor空间的比例为8:1，对象分配到Eden和其中一个
Survivor空间，回收时将存活的对象复制到另一个Survivor空间，然后将Eden空间和先前
使用的Survivor空间清理。
老年代因对象生命周期较长，每次回收只有少部分对象没清理，如果使用“复制-清理”算法的话需要额外预留更多的空闲空间用于复制生存对象，
（ 例如，100M的老年代占用空间 每次能回收50% ，那么他需要预留50M的空间  内存使用上不经济 ）
所以回收时使用“标记-整理”算法。



## 年轻代为什么被划分成eden、survivor区域？

 HotSpot JVM把年轻代分为了三部分：1个Eden区和2个Survivor区（分别叫from和to）。默认比例为8：1,为啥默认会是这个比例，接下来我们会聊到。一般情况下，新创建的对象都会被分配到Eden区(一些大对象特殊处理),这些对象经过第一次Minor GC后，如果仍然存活，将会被移到Survivor区。对象在Survivor区中每熬过一次Minor GC，年龄就会增加1岁，当它的年龄增加到一定程度时，就会被移动到年老代中。

因为年轻代中的对象基本都是朝生夕死的(80%以上)，所以在年轻代的垃圾回收算法使用的是复制算法，复制算法的基本思想就是将内存分为两块，每次只用其中一块，当这一块内存用完，就将还活着的对象复制到另外一块上面。复制算法不会产生内存碎片。

在GC开始的时候，对象只会存在于Eden区和名为“From”的Survivor区，Survivor区“To”是空的。紧接着进行GC，Eden区中所有存活的对象都会被复制到“To”，而在“From”区中，仍存活的对象会根据他们的年龄值来决定去向。年龄达到一定值(年龄阈值，可以通过-XX:MaxTenuringThreshold来设置)的对象会被移动到年老代中，没有达到阈值的对象会被复制到“To”区域。经过这次GC后，Eden区和From区已经被清空。这个时候，“From”和“To”会交换他们的角色，也就是新的“To”就是上次GC前的“From”，新的“From”就是上次GC前的“To”。不管怎样，都会保证名为To的Survivor区域是空的。Minor GC会一直重复这样的过程，直到“To”区被填满，“To”区被填满之后，会将所有对象移动到年老代中。

![young_gc](http://ifeve.com/wp-content/uploads/2014/07/young_gc.png)

## java动态代理和cglib动态代理的区别？

**jdk动态代理**

**特点**

1. Interface：对于JDK Proxy,业务类是需要一个Interface的，这是一个缺陷；
2. Proxy：Proxy类是动态产生的，这个类在调用Proxy.newProxyInstance()方法之后，产生一个Proxy类的实力。实际上，这个Proxy类也是存在的，不仅仅是类的实例，这个Proxy类可以保存在硬盘上；
3. Method：对于业务委托类的每个方法，现在Proxy类里面都不用静态显示出来。
4. InvocationHandler：这个类在业务委托类执行时，会先调用invoke方法。invoke方法在执行想要的代理操作，可以实现对业务方法的再包装。

**总结**：

- JDK动态代理类实现了InvocationHandler接口，重写的invoke方法。
- JDK动态代理的基础是反射机制（method.invoke(对象，参数)）Proxy.newProxyInstance()

##### cglib动态代理

**特点**

1. 原理是对指定的目标生成一个子类，并覆盖其中方法实现增强，但因为采用的是继承，所以不能对final修饰的类进行代理。

**注意**:jdk的动态代理只可以为接口去完成操作，而cglib它可以为没有实现接口的类去做代理，也可以为实现接口的类去做代理。



## Dubbo如何做负载均衡？

Dubbo实现负载均衡，一般是对服务提供者进行集群，服务消费者在请求消费时，通过一定的算法进行寻址（权重），跟nginx等做法差不多。

具体做法，对服务提供者的配置文件，Dubbo 任务application name相同则认为是同一集群。部署多个同一集群的不同端口服务即可。

<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xmlns="http://www.springframework.org/schema/beans"
    xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd
       http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd">

    <!-- 提供方应用名称, 用于计算依赖关系 -->
    <dubbo:application name="demo-provider" />
    
    <!-- 使用zookeeper注册中心暴露服务地址 -->
    <dubbo:registry address="zookeeper://127.0.0.1:2181" />
    
    <!-- 使用dubbo协议在20880端口暴露服务 -->
    <dubbo:protocol name="dubbo" port="20880" />
    
    <!-- service实现类作为本地的一个bean -->
    <bean id="demoProviderService" class="com.hbk.dubbo.DemoProviderServiceImpl" />
    
    <!-- 声明需要暴露的服务接口 -->
    <dubbo:service interface="com.hbk.dubbo.DemoProviderService"
        ref="demoProviderService" />
</beans>





## Dubbo如何做限流降级？

1. dubbo的服务者与消费者  service的配置，最大连接数 与 请求数目配置

2. dubbo的超时设置 + 配置mock 类。 请求超时后会执行mock，并返回
3. dubbo可以通过扩展Filter的方式引入Hystrix，具体代码如下：

https://github.com/yskgood/dubbo-hystrix-support

## Dubbo如何优雅的下线服务？

能实现优雅停机的前提是，在启动时，需要指定参数-Ddubbo.shutdown.hook=true

Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果使用 kill -9 PID 等强制关闭指令，是不会执行优雅停机的，只有通过 kill PID 时，才会执行。



## Dubbo如何实现异步调用的？

1. 当使用异步调用时建议要和原有api进行区别，即将同步调用和异步调用的api接口分离

![1578537752000](.\img\1578537752000.png)

2.api注入时添加异步调用标示

![1578537850404](./img\1578537850404.png)

3.在网关启动类开启异步调用

![1578537771611](./img\1578537771611.png)

4.在网关为需要异步调用的接口添加异步调用代码

![1578537945865](./img\1578537945865.png)

## RocketMq如何保证高可用的？

高可用HA（High Availability）是分布式系统架构设计中必须考虑的因素之一，
它通常是指，通过设计减少系统不能提供服务的时间。
假设系统一直能够提供服务，我们说系统的可用性是100%。
如果系统每运行100个时间单位，会有1个时间单位无法提供服务，我们说系统的可用性是99%。
现在公司的服务基本都是水平+垂直拆分,在降低服务间耦合度的同时增加了系统的高可用

<table>
    <tr><th>部署方式</th><th>优点</th><th>缺点</th><th>备注</th></tr>
	<tr><td>单个Master模式</td><td>一旦Broker重启或者宕机时，会导致整个服务不可用，不建议线上环境使用；</td><td></td><td></td></tr>
    <tr><td>多个Master模式</td><td>配置简单，单个Master宕机或重启维护对应用无影响，在磁盘配置为RAID10时，即使机器宕机不可恢复情况下，由于RAID10磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条不丢），性能最高。</td><td>单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会收到影响。</td><td>当使用多master无slave的集群搭建方式时，master的brokerRole配置必须为ASYNC_MASTER。如果配置为SYNC_MASTER，则producer发送消息时，返回值的SendStatus会一直是SLAVE_NOT_AVAILABLE。</td></tr>
    <tr><td>多Master多Slave模式——异步复制	</td><td>即使磁盘损坏，消息丢失的非常少，但消息实时性不会受影响，因为Master宕机后，消费者仍然可以从Slave消费，此过程对应用透明，不需要人工干预，性能同多Master模式几乎一样。</td><td>Master宕机，磁盘损坏情况，会丢失少量信息。</td><td></td></tr>
    <tr><td>多Master多Slave模式——同步双写</td><td>数据与服务都无单点，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高；</td><td></td><td>性能比异步复制模式稍低，大约低10%左右，发送单个消息的RT会稍高，目前主宕机后，备机不能自动切换为主机，后续会支持自动切换功能。</td></tr>
</table>

### 建议

由于公司数据对高可用要求比较高，采用多master，多slave模式–同步双写，虽然虽然会降低效率，增加运营成功，但是对于系统解耦和可用性来说是值得的。

## RocketMq如何保证高吞吐的？

half消息和commit在存储层面都是顺序写文件的， commit是one way，通信开销小。

## RocketMq的消息是有序的吗？

我们知道Topic的有序消息已经成为mq的标配。而RocketMQ中是这样区分消息类型的， 普通消息也叫做无序消息，简单来说就是没有顺序的消息，而有序消息就是按照一定的先后顺序的消息类型。举个例子，producer 依次发送 order id 为 1、2、3 的消息到 broker，consumer 接到的消息顺序也就是 1、2、3 ，而不会出现普通消息那样的 2、1、3 等情况。

## redis工作模型以及淘汰机制是什么？

单线程工作模型.

淘汰机制:

redis采用的是定期删除+惰性删除策略。
为什么不用定时删除策略?
定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略.
定期删除+惰性删除是如何工作的呢?
定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。
于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。
采用定期删除+惰性删除就没其他问题了么?
不是的，如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制。
在redis.conf中有一行配置

````
maxmemory-policy volatile-lru
````



该配置就是配内存淘汰策略的(什么，你没配过？好好反省一下自己)
1）noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。应该没人用吧。
2）allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。推荐使用，目前项目在用这种。
3）allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。应该也没人用吧，你不删最少使用Key,去随机删。
4）volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。这种情况一般是把redis既当缓存，又做持久化存储的时候才用。不推荐
5）volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。依然不推荐
6）volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。不推荐
ps：如果没有设置 expire 的key, 不满足先决条件(prerequisites); 那么 volatile-lru, volatile-random 和 volatile-ttl 策略的行为, 和 noeviction(不删除) 基本上一致。

## 单线程的redis如何利用多核cpu机器？

redis的读取和处理性能非常强大，一般服务器的cpu都不会是性能瓶颈。redis的性能瓶颈主要集中在内存和网络方面。所以，如果使用的redis命令多为O(N)、O(log(N))时间复杂度，那么基本上不会出现cpu瓶颈的情况。
但是如果你确实需要充分使用多核cpu的能力，那么需要在单台服务器上运行多个redis实例(主从部署/集群化部署)，并将每个redis实例和cpu内核进行绑定



## redis集群有哪几种形式？

redis集群有三种方式：主从复制，哨兵，集群（master-cluster）。

1.主从复制（Master-Slave）
数据库分为两类，Master数据库和Slave数据库。
有以下特点：
1.master一般是接受读写的，slave只接受读操作。
2.当master接受写操作后会将命令发送给slave执行，从而实现数据一致性
3.一个master下面可以有多个slave，但是一个slave上面只能有一个master
原理：
1 从服务器连接主服务器发送sync命令。
2 主服务器持久化数据生成rdb文件并缓存这段时间的写命令
3 主服务器向从服务器发送rdb文件和缓存的命令
4 从服务器载入rdb快照后执行缓存的命令（从服务器初始化完成）
5 主服务器每接收到一个写命令就发送给从服务器执行（从服务器初始化完成后的操作）

2 .哨兵模式
哨兵是用来监控主从数据库的，当master挂掉后选择一个salve当做master。

与master建立连接后，哨兵会执行三个操作，这三个操作的发送频率都可以在配置文件中配置：

定期向master和slave发送INFO命令
定期向master个slave的_sentinel_:hello频道发送自己的信息
定期向master、slave和其他哨兵发送PING命令

master挂掉后从slave中选取规则
1.所有在线的slave中选择优先级最高的，优先级可以通过slave-priority配置
2.如果有多个最高优先级的slave，则选取复制偏移量最大（即复制越完整）的当选
3.如果以上条件都一样，选取id最小的slave

3.集群：
redis的哨兵模式基本已经可以实现高可用，读写分离 ，但是在这种模式下每台redis服务器都存储相同的数据，很浪费内存，所以在redis3.0上加入了cluster模式，实现的redis的分布式存储，也就是说每台redis节点上存储不同的内容。
使用集群，只需要将每个数据库节点的cluster-enable配置打开即可。每个集群中至少需要三个主数据库才能正常运行。

## redis有哪几种数据结构？

五种:String , Hash , List , Sorted Set , Set,对五种数据结构的解释解释的话我认为官方的解释是最有力的:www.redis.cn

 

## Mysql不同事务隔离级别分别会加哪些锁？

共享锁(Shared lock)

例1：

----------------------------------------

T1: select * from table (请想象它需要执行1个小时之久，后面的sql语句请都这么想象）

T2: update table set column1='hello'

 

过程：T1运行 （加共享锁)

T2运行等待T1运行完之后再运行T2

 

之所以要等，是因为T2在执行update前，试图对table表加一个排他锁，而数据库规定同一资源上不能同时共存共享锁和排他锁。所以T2必须等T1执行完，释放了共享锁，才能加上排他锁，然后才能开始执行update语句。

 

例2：

----------------------------------------

T1: select * from table

T2: select * from table

 

这里T2不用等待T1执行完，而是可以马上执行。

 

分析：

 

T1运行，则table被加锁，比如叫lockA

T2运行，再对table加一个共享锁，比如叫lockB。

 

两个锁是可以同时存在于同一资源上的（比如同一个表上）。这被称为共享锁与共享锁兼容。这意味着共享锁不阻止其它session同时读资源，但阻止其它session update

 

例3：

----------------------------------------

T1: select * from table

T2: select * from table

T3: update table set column1='hello'

 

这次，T2不用等T1运行完就能运行，T3却要等T1和T2都运行完才能运行。因为T3必须等T1和T2的共享锁全部释放才能进行加排他锁然后执行update操作。

 

例4：（死锁的发生）

----------------------------------------

 

T1:begin tran

select * from table (holdlock) (holdlock意思是加共享锁，直到事务结束才释放)

update table set column1='hello'

 

T2:begin tran

select * from table(holdlock)

update table set column1='world'

 

假设T1和T2同时达到select，T1对table加共享锁，T2也对加共享锁，当T1的select执行完，准备执行update时，根据锁机制，T1的共享锁需要升级到排他锁才能执行接下来的update.在升级排他锁前，必须等table上的其它共享锁释放，但因为holdlock这样的共享锁只有等事务结束后才释放，所以因为T2的共享锁不释放而导致T1等(等T2释放共享锁，自己好升级成排他锁），同理，也因为T1的共享锁不释放而导致T2等。死锁产生了。

 

例5：

----------------------------------------

T1:begin tran

update table set column1='hello' where id=10

 

T2:begin tran

update table set column1='world' where id=20

 

这种语句虽然最为常见，很多人觉得它有机会产生死锁，但实际上要看情况，如果id是主键上面有索引，那么T1会一下子找到该条记录(id=10的记录），然后对该条记录加排他锁，T2，同样，一下子通过索引定位到记录，然后对id=20的记录加排他锁，这样T1和T2各更新各的，互不影响。T2也不需要等。

 

但如果id是普通的一列，没有索引。那么当T1对id=10这一行加排他锁后，T2为了找到id=20，需要对全表扫描，那么就会预先对表加上共享锁或更新锁或排他锁(依赖于数据库执行策略和方式，比如第一次执行和第二次执行数据库执行策略就会不同）。但因为T1已经为一条记录加了排他锁，导致T2的全表扫描进行不下去，就导致T2等待。

 

死锁怎么解决呢？一种办法是，如下：

例6：

---------------------------------------

T1:begin tran

select * from table(xlock) (xlock意思是直接对表加排他锁)

update table set column1='hello'

 

T2:begin tran

select * from table(xlock)

update table set column1='world'

 

这样，当T1的select 执行时，直接对表加上了排他锁，T2在执行select时，就需要等T1事务完全执行完才能执行。排除了死锁发生。但当第三个user过来想执行一个查询语句时，也因为排他锁的存在而不得不等待，第四个、第五个user也会因此而等待。在大并发情况下，让大家等待显得性能就太友好了，所以，这里引入了更新锁。

 

更新锁(Update lock)

为解决死锁，引入更新锁

 

例7：

----------------------------------------

T1:begin tran

select * from table(updlock) (加更新锁)

update table set column1='hello'

 

T2:begin tran

select * from table(updlock)

update table set column1='world'

 

更新锁的意思是：“我现在只想读，你们别人也可以读，但我将来可能会做更新操作，我已经获取了从共享锁（用来读）到排他锁（用来更新）的资格”。一个事务只能有一个更新锁获此资格。

 

T1执行select，加更新锁。

T2运行，准备加更新锁，但发现已经有一个更新锁在那儿了，只好等。（T2加的是更新锁，更新锁与更新锁不兼容， 如果加的是共享锁， 共享锁和更新锁可以兼容，即T1，T2不可同时进行，但是T3，T4，T5只要不是事务，还是可以正常查询）

 

当后来有user3、user4...需要查询table表中的数据时，并不会因为T1的select在执行就被阻塞，照样能查询，相比起例6，这提高了效率

 

例8:

----------------------------------------

T1:begin

select * from table(updlock) (加更新锁）

update table set column1='hello' (重点：这里T1做update时，不需要等T2释放什么，而是直接把更新锁升级为排他锁，然后执行update)

 

T2:begin

select * from table (T1加的更新锁不影响T2读取）

update table set column1='world' (T2的update需要等T1的update做完才能执行)

 

我们以这个例子来加深更新锁的理解，

 

第一种情况：T1先达，T2紧接到达；在这种情况中，T1先对表加更新锁，T2对表加共享锁，假设T2的select先执行完，准备执行update，发现已有更新锁存在，T2等。T1执行这时才执行完select，准备执行update，更新锁升级为排他锁，然后执行update，执行完成，事务结束，释放锁，T2才轮到执行update。

 

第二种情况：T2先达，T1紧接达；在这种情况，T2先对表加共享锁，T1达后，T1对表加更新锁，假设T2 select先结束，准备update，发现已有更新锁，则等待，后面步骤就跟第一种情况一样了。这个例子是说明：排他锁与更新锁是不兼容的，它们不能同时加在同一子资源上。

 

排他锁（独占锁，Exclusive Locks)

这个简单，即其它事务既不能读，又不能改排他锁锁定的资源。

例9

T1: update table set column1='hello' where id<1000

T2: update table set column1='world' where id>1000

 

假设T1先达，T2随后至，这个过程中T1会对id<1000的记录施加排他锁.但不会阻塞T2的update。

 

例10 (假设id都是自增长且连续的）

T1: update table set column1='hello' where id<1000

T2: update table set column1='world' where id>900

 

如同例9，T1先达，T2立刻也到，T1加的排他锁会阻塞T2的update.

 

意向锁(Intent Locks)

意向锁就是说在屋（比如代表一个表）门口设置一个标识，说明屋子里有人（比如代表某些记录）被锁住了。另一个人想知道屋子里是否有人被锁，不用进屋子里一个一个的去查，直接看门口标识就行了。

 

当一个表中的某一行被加上排他锁后，该表就不能再被加表锁。数据库程序如何知道该表不能被加表锁？一种方式是逐条的判断该表的每一条记录是否已经有排他锁，另一种方式是直接在表这一层级检查表本身是否有意向锁，不需要逐条判断。显然后者效率高。

 

例11：

----------------------------------------

T1: begin tran

select * from table (xlock) where id=10 --意思是对id=10这一行强加排他锁

T2: begin tran

select * from table (tablock) --意思是要加表级锁  

 

假设T1先执行，T2后执行，T2执行时，欲加表锁，为判断是否可以加表锁，数据库系统要逐条判断table表每行记录是否已有排他锁，如果发现其中一行已经有排他锁了，就不允许再加表锁了。只是这样逐条判断效率太低了。

 

实际上，数据库系统不是这样工作的。当T1的select执行时，系统对表table的id=10的这一行加了排他锁，还同时悄悄的对整个表加了意向排他锁(IX)，当T2执行表锁时，只需要看到这个表已经有意向排他锁存在，就直接等待，而不需要逐条检查资源了。

 

 

计划锁(Schema Locks)

例12：

----------------------------------------

alter table .... (加schema locks，称之为Schema modification (Sch-M) locks

 

DDL语句都会加Sch-M锁，该锁不允许任何其它session连接该表。连都连不了这个表了，当然更不用说想对该表执行什么sql语句了。

 

事务隔离级别（MySQL仅InnoDB引擎支持事务，所以也只有InnoDB有事务隔离级别）

Read Uncommit (未提交读。允许脏读)

事例：老板要给程序员发工资，程序员的工资是3.6万/月。但是发工资时老板不小心按错了数字，按成3.9万/月，该钱已经打到程序员的户口，但是事务还没有提交，就在这时，程序员去查看自己这个月的工资，发现比往常多了3千元，以为涨工资了非常高兴。但是老板及时发现了不对，马上回滚差点就提交了的事务，将数字改成3.6万再提交。

分析：实际程序员这个月的工资还是3.6万，但是程序员看到的是3.9万。他看到的是老板还没提交事务时的数据。这就是脏读。

 

一级封锁协议：

一级封锁协议，事务在对需要修改的数据上面（就是在发生修改的瞬间） 对其加共享锁（其他事务不能更改，但是可以读取-导致“脏读”），直到事务结束才释放。

 

Read committed（读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。）

事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（程序员事务开启），收费系统事先检测到他的卡里有3.6万，就在这个时候！！程序员的妻子要把钱全部转出充当家用，并提交。当收费系统准备扣款时，再检测卡里的金额，发现已经没钱了（第二次检测金额当然要等待妻子转出金额事务提交完）。程序员就会很郁闷，明明卡里是有钱的…

分析：这就是读提交，若有事务对数据进行更新（UPDATE）操作时，读操作事务要等待这个更新操作事务提交后才能读取数据，可以解决脏读问题。但在这个事例中，出现了一个事务范围内两个相同的查询却返回了不同数据，这就是不可重复读。

 

二级封锁协议：

1）事务 在对需要更新的数据 上（就是发生更新的瞬间） 加 排他锁 （直到事务结束） ， 防止其他事务读取未提交的数据，这样，也就避免了 “脏读” 的情况。2）事务 对当前被读取的数据 上面加共享锁（当读到时加上共享锁），一旦读完该行，立即 释放该 该行的共享锁

     二级封锁协议除防止了“脏读”数据，但是不能避免 丢失更新，不可重复读，幻读 。
    
     但在二级封锁协议中，由于读完数据后立即 释放共享锁，所以它不能避免可重复读 ，同时它也不能避免 丢失更新 ，如果事务A、B同时获取资源X，然后事务A先发起更新记录X，那么 事务B 将等待事务 A 执行完成，然后获得记录X 的排他锁，进行更改。这样事务 A 的更新将会被丢失。

具体情况如下：

 

事务A

事务B

读取X=100（同时上共享锁）

读取X=100（同时上共享锁）

读取成功（释放共享锁）

读取成功（释放共享锁）

UPDATE X=X+100 （上排他锁）

 

 

UPDATING A（等待事务A释放对X的排他锁）

事务成功（释放排他锁）X=200

 

 

UPDATE X=X+200（成功上排他锁）

 

事务成功（释放排他锁）X=300

 

由此可以看到，事务A的提交被事务B覆盖了

 

Repeatable read（重复读，就是在开始读取数据（事务开启）时，不再允许修改操作）

事例：程序员拿着信用卡去享受生活（卡里当然是只有3.6万），当他埋单时（事务开启，不允许其他事务的UPDATE修改操作），收费系统事先检测到他的卡里有3.6万。这个时候他的妻子不能转出金额了。接下来收费系统就可以扣款了。

分析：重复读可以解决不可重复读问题。写到这里，应该明白的一点就是，不可重复读对应的是修改，即UPDATE操作。但是可能还会有幻读问题。因为幻读问题对应的是插入INSERT操作，而不是UPDATE操作。

 

什么时候会出现幻读？

事例：程序员某一天去消费，花了2千元，然后他的妻子去查看他今天的消费记录（全表扫描FTS，妻子事务开启），看到确实是花了2千元，就在这个时候，程序员花了1万买了一部电脑，即新增INSERT了一条消费记录，并提交。当妻子打印程序员的消费记录清单时（妻子事务提交），发现花了1.2万元，似乎出现了幻觉，这就是幻读。

 

三级封锁协议：

三级封锁协议是：二级封锁协议加上事务 在读取数据的瞬间 必须先对其加 共享锁 ，但是 直到事务结束才释放 ，这样保证了可重复读（既是其他的事务职能读取该数据，但是不能更新该数据）。

三级封锁协议除防止了“脏”数据 和不可重复读 。但是这种情况不能避免 幻读 和 丢失更新 的情况，在事务 A 没有完成之前，事务 B 可以新增数据，那么 当事务 A 再次读取的时候，事务B 新增的数据会被读取到

 

进阶：repeatable read 导致死锁的情况，参考上面讲解共享锁的文章

 

Serializable 序列化

Serializable 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。

 

四级封锁协议：

四级封锁协议是对三级封锁协议的增强，其实现机制也最为简单，直接对 事务中 所 读取 或者 更改的数据所在的表加表锁，也就是说，其他事务不能 读写 该表中的任何数据。这样所有的 脏读，不可重复读，幻读 ，都得以避免！

 

值得一提的是：大多数数据库默认的事务隔离级别是Read committed，比如Sql Server , Oracle。Mysql的默认隔离级别是Repeatable read。

 

乐观锁（ Optimistic Locking ） 相对悲观锁而言，乐观锁机制采取了更加宽松的加锁机制。悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性。但随之而来的就是数据库性能的大量开销，特别是对长事务而言，这样的开销往往无法承受。而乐观锁机制在一定程度上解决了这个问题。乐观锁，大多是基于数据版本（ Version ）记录机制实现。何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 “version” 字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。

悲观锁（Pessimistic Lock），正如其名，具有强烈的独占和排他特性。它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。

事务的特性： 

1.原子性  事务是数据库逻辑的工作单元，事务包括的所有操作，要么都做，要么都不做。 

2.一致性  事务执行的结果是使数据库从一个一致性状态变成另一个一致性状态。一致性与原子性是密切相关的。 

3.隔离性  一个事务的执行不能被其他事务干扰。 

4.持久性  一个事务一旦提交，它对数据库中数据的改变应该是永久性的。

## mysql的行锁、表锁、间隙锁、意向锁分别是做什么的？

**MYISAM表锁**

MySQL的表级锁有两种模式：表共享读锁（Table Read Lock）和表独占写锁（Table Write Lock）。
对MyISAM表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；对 MyISAM表的写操作，则会阻塞其他用户对同一表的读和写操作；MyISAM表的读操作与写操作之间，以及写操作之间是串行的！当一个线程获得对一个表的写锁后，只有持有锁的线程可以对表进行更新操作。其他线程的读、写操作都会等待，直到锁被释放为止。

**行锁**

InnoDB实现了以下两种类型的行锁。

共享锁（s）：又称读锁。允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。若事务T对数据对象A加上S锁，则事务T可以读A但不能修改A，其他事务只能再对A加S锁，而不能加X锁，直到T释放A上的S锁。这保证了其他事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。
排他锁（Ｘ）：又称写锁。允许获取排他锁的事务更新数据，阻止其他事务取得相同的数据集共享读锁和排他写锁。若事务T对数据对象A加上X锁，事务T可以读A也可以修改A，其他事务不能再对A加任何锁，直到T释放A上的锁。
**间隙锁**

当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的 索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁 （Next-Key锁）